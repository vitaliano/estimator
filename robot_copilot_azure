"""
0 em AZURE leia client-location uniques em login_camera onde comments='c' (cameras a corrigir)
1 para cada par client-location  
    1-1 se o db client-location.db nao existir, crie-o
    1-2 atualizar peopleflowtotals.db com os dados mais recentes do sql server azure
    1-3 encontre falhas de camera em peopleflowtotals.db(cameras com falhas de contagem) salve as falhas em uma lista
    1-4 para cada falha de camera, encontre a correcao adequada - salve as correcoes em uma lista
    1-5 encontre as falhas de contagem negativa e corrija interativamente( running sum ins< outs) salve as correcoes em uma lista
    1-6 veja em  azure.allclients table o tipo de correcao desejado para este client-location (cameras ruins ou cameras ruins e negativos)
    1-6 se cameras ruins apenas, aplique as correcao de cameras ruins
    1-7 se cameras ruins e negativos, aplique as correcao de cameras



# ------------------------------
# 2. Logging Helpers
# ------------------------------
LOGFILE = "camera_failure_and_correction_log.csv"
def init_log():
   Initialize log file with headers if not present.
    if not os.path.exists(LOGFILE):
        with open(LOGFILE, mode="w", newline="") as f:
            writer = csv.writer(f)
            writer.writerow([
                "timestamp",
                "client",
                "location",
                "camera_id",
                "created_at",
                "fail_type",
                "correction_type", # can be increase, decrease on ins or outs
                "correction_method", # can be by proportion or by history
                "value_before_correction",
                "value_after_correction",
            ])

def log_fail(client, location, cam_id, ts, fail_type, correction_type,
                   correction_method,value_before_coorection,value_after_correction,correction_date):
    Append one imputation record to the log file
    with open(LOGFILE, mode="a", newline="") as f:
        writer = csv.writer(f)
        writer.writerow([
            datetime.now().isoformat(),
            client,
            location,
            cam_id,
            ts,
            fail_type,
            correction_type,
            correction_method,
            value_before_coorection,
            value_after_correction,
            correction_date
        ])


# ------------------------------
# 3. Get Client_Location list
# ------------------------------
def get_client_locations(conn):
    # o campo comments em login camera indica que esta camera deve ser corrigida
    query="SELECT DISTINCT client, location FROM login_camera WHERE comments ='c' ;"
    client_locations=conn.execute(query).fetchall()
    return client_locations


# ------------------------------
# Main Processing
# ------------------------------


def main():

    init_log()
    client_locations = get_client_locations(conn)
    for client, location in client_locations:
        failures=find_failures(conn, client, location)
        corrections=find_corrections(conn, failures)
        for correction in corrections:
                apply_correction(conn, correction)
                log_fail(
                    client,
                    location,
                    correction['camera_id'],
                    correction['created_at'],
                    correction['fail_type'],
                    correction['correction_type'],
                    correction['correction_method'],
                    correction['value_before_correction'],
                    correction['value_after_correction'],
                    correction['correction_date']
                )
        negative_test=get_non_negative_test(conn, client, locatio
conn.close()

if __name__ == "__main__":
    main()


"""



import pyodbc
import pandas as pd
import numpy as np
import csv
import os
import re
import sqlite3
from datetime import datetime, timedelta


# ---------------------------------------------------------
# 1. CONFIGURACAO DO SQL SERVER AZURE
# ---------------------------------------------------------
server = 'm4n1182kyd.database.windows.net'
database = 'nodehub'
username = 'nodehub-3rcorp'
password = 'doutorCHAPATA@123'
driver = '{ODBC Driver 17 for SQL Server}'  # Make sure this driver is installed

az_conn = pyodbc.connect(
    f'DRIVER={driver};SERVER={server};DATABASE={database};UID={username};PWD={password}'
)
az_cursor = az_conn.cursor()



# ---------------------------------------------------------
# 2. SYNCHRONIZE  SQLITE LOCAL nodehub-client-server.db
# ---------------------------------------------------------
def synchro_sqlite_az(client,location,az_conn,az_cursor):
    c_client= re.sub(r'[^A-Za-z0-9]', '', client)
    c_location= re.sub(r'[^A-Za-z0-9]', '', location)
    db_name=   f"{c_client}-{c_location}-peopleflowtotals.db"
    
    try:
        li_conn = sqlite3.connect(db_name)
        li_cursor = li_conn.cursor()

        # 0. Find all cams for this client-location in azure db
        az_cursor.execute(""" SELECT DISTINCT camera_id
            FROM login_camera
            WHERE client = ? AND location = ? AND comments = 'c'
        """, (client, location))
        cameras = [row[0] for row in az_cursor.fetchall()]

        # 1. create peopleflowtotals table if not yet exists
        li_cursor.execute("""
        CREATE TABLE IF NOT EXISTS peopleflowtotals (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            created_at DATETIME,
            camera_id INTEGER,
            total_inside INTEGER,
            total_outside INTEGER,
            valid INTEGER
        );
        """)

        # 2. Find the most recent record in sqlite db
        li_cursor.execute("""
            SELECT MAX(created_at) FROM peopleflowtotals
        """)
        max_created_at_li = li_cursor.fetchone()[0]

        # 3. Find the most recent record in azure db
        az_cursor.execute("""
            SELECT MAX(created_at) FROM peopleflowtotals
                WHERE camera_id IN ({','.join('?' * len(cameras))}) 
                AND VALID=1
                ORDER BY created_at ASC
            """, cameras)
        max_created_at_az = az_cursor.fetchone()[0]
    
        # 4. Find di  to import from azure
        if max_created_at_li is None:
            di = max_created_at_az - timedelta(days=100)
        else: 
            di = max_created_at_li + timedelta(days=1)

        # 5. import from azure record when  dates more recent  than di
        az_cursor.execute("""
            SELECT *
            FROM peopleflowtotals
            WHERE camera_id IN ({','.join('?' * len(cameras))})
            AND created_at > ?
            ORDER BY created_at ASC
        """, cameras,di)
        rows = az_cursor.fetchall()

        # 6. Insert those records at azure
        for row in rows:
            li_cursor.execute("""
                INSERT OR REPLACE INTO peopleflowtotals 
                (created_at,camera_id,total_inside,total_outside,valid)
                VALUES (?, ?, ?, ?, ?)
            """, (row.created_at, row.camera_id, row.total_inside, row.total_outside, row.valid))
            
        # 7. Select all records in local db to df_peopleflow
        query="""
            SELECT *
            FROM peopleflowtotals
            WHERE camera_id IN ({','.join('?' * len(cameras))})
            ORDER BY created_at ASC
        """)
        df_peopleflowtotals=pd.read_sql_query(query, li_conn, params=(cameras))
        li_conn.commit



        return cameras,df_peopleflowtotals,max_created_at_az
    
    except:
        return None










