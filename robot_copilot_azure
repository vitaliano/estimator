"""
0 em AZURE leia client-location uniques em login_camera onde comments='c' (cameras a corrigir)
1 para cada par client-location  
    1-1 se o db client-location.db nao existir, crie-o
    1-2 atualizar peopleflowtotals.db com os dados mais recentes do sql server azure
    1-3 encontre falhas de camera em peopleflowtotals.db(cameras com falhas de contagem) salve as falhas em uma lista
    1-4 para cada falha de camera, encontre a correcao adequada - salve as correcoes em uma lista
    1-5 encontre as falhas de contagem negativa e corrija interativamente( running sum ins< outs) salve as correcoes em uma lista
    1-6 veja em  azure.allclients table o tipo de correcao desejado para este client-location (cameras ruins ou cameras ruins e negativos)
    1-6 se cameras ruins apenas, aplique as correcao de cameras ruins
    1-7 se cameras ruins e negativos, aplique as correcao de cameras



# ------------------------------
# 2. Logging Helpers
# ------------------------------
LOGFILE = "camera_failure_and_correction_log.csv"
def init_log():
   Initialize log file with headers if not present.
    if not os.path.exists(LOGFILE):
        with open(LOGFILE, mode="w", newline="") as f:
            writer = csv.writer(f)
            writer.writerow([
                "timestamp",
                "client",
                "location",
                "camera_id",
                "created_at",
                "fail_type", # can be min qty input, max_qty_output, max_qty_input, max_qty_output, rec absent, negative_total, excessive_total
                "correction_type", # can be update, insert on ins or outs 
                "correction_method", # can be by proportion or by history or by percentage (in case of negative or excessive)
                "value_before_correction",
                "value_after_correction",
            ])

def log_fail(client, location, cam_id, ts, fail_type, correction_type,
                   correction_method,value_before_coorection,value_after_correction,correction_date):
    Append one imputation record to the log file
    with open(LOGFILE, mode="a", newline="") as f:
        writer = csv.writer(f)
        writer.writerow([
            datetime.now().isoformat(),
            client,
            location,
            cam_id,
            ts,
            fail_type,
            correction_type,
            correction_method,
            value_before_coorection,
            value_after_correction,
            correction_date
        ])


# ------------------------------
# 3. Get Client_Location list
# ------------------------------
def get_client_locations(conn):
    # o campo comments em login camera indica que esta camera deve ser corrigida
    query="SELECT DISTINCT client, location FROM login_camera WHERE comments ='c' ;"
    client_locations=conn.execute(query).fetchall()
    return client_locations


# ------------------------------
# Main Processing
# ------------------------------



if __name__ == "__main__":
    main()


"""



import pyodbc
import pandas as pd
import numpy as np
import csv
import os
import re
import sqlite3
from datetime import datetime, timedelta


# ---------------------------------------------------------
# 1. CONFIGURACAO DO SQL SERVER AZURE
# ---------------------------------------------------------
server = 'm4n1182kyd.database.windows.net'
database = 'nodehub'
username = 'nodehub-3rcorp'
password = 'doutorCHAPATA@123'
driver = '{ODBC Driver 17 for SQL Server}'  # Make sure this driver is installed

def get_client_location_list():
    try:
        az_conn = pyodbc.connect(
            f'DRIVER={driver};SERVER={server};DATABASE={database};UID={username};PWD={password}'
        )
        az_cursor = az_conn.cursor()
        client_locations=az_conn.execute("SELECT DISTINCT client, location FROM login_camera;").fetchall()
        return az_conn, az_cursor, client_locations
    except:
        return None


# ---------------------------------------------------------
# 2. LOGS HELPER
# ---------------------------------------------------------
LOGFILE = "camera_failure_and_correction_log.csv"
def init_log():
    if not os.path.exists(LOGFILE):
        with open(LOGFILE, mode="w", newline="") as f:
            writer = csv.writer(f)
            writer.writerow([
                "timestamp",
                "client",
                "location",
                "camera_id",
                "created_at",
                "fail_type", # can be min qty input, max_qty_output, max_qty_input, max_qty_output, rec absent, negative_total, excessive_total
                "correction_type", # can be update_ins, update_outs,insert_rec
                "correction_method", # can be by proportion or by history or by percentage (in case of negative or excessive)
                "input_before",
                "input_after",
                "output_before",
                "output_after"
            ])

def register_log(client, location, cam_id, ts, fail_type, correction_type,
                   correction_method, input_before, input_after,output_before,output_after):
    """Append one imputation record to the log file."""
    with open(LOGFILE, mode="a", newline="") as f:
        writer = csv.writer(f)
        writer.writerow([
            datetime.now().isoformat(),
            client,
            location,
            cam_id,
            ts,
            fail_type,
            correction_type,
            correction_method,
            input_before,
            input_after,
            output_before,
            output_after
        ])


# ---------------------------------------------------------
# 3. SYNCHRONIZE SQLITE LOCAL com AZURE AND GET DFs
# ---------------------------------------------------------
def synchro_sqlite_az(client,location,az_cursor):
    c_client= re.sub(r'[^A-Za-z0-9]', '', client)
    c_location= re.sub(r'[^A-Za-z0-9]', '', location)
    db_name=   f"{c_client}-{c_location}-peopleflowtotals.db"
    
    try:
        li_conn = sqlite3.connect(db_name)
        li_cursor = li_conn.cursor()

        # 0. Find all cams for this client-location in azure db
        query = """ 
            SELECT DISTINCT camera_id
            FROM login_camera
            WHERE client = ? AND location = ? AND comments = 'c'
        """
        df_login_cameras=pd.read_sql_query(query, li_conn,  params=[client, location])
        camera_ids = df_login_cameras['id'].tolist()
        camera_ids_str = ','.join(map(str, camera_ids))


        # 1. create peopleflowtotals table if not yet exists
        li_cursor.execute("""
        CREATE TABLE IF NOT EXISTS peopleflowtotals (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            created_at DATETIME,
            camera_id INTEGER,
            total_inside INTEGER,
            total_outside INTEGER,
            valid INTEGER
        );
        """)

        # 2. Find the most recent record in sqlite db
        li_cursor.execute("""
            SELECT MAX(created_at) FROM peopleflowtotals
        """)
        max_created_at_li = li_cursor.fetchone()[0]

        # 3. Find the most recent record in azure db
        az_cursor.execute("""
            SELECT MAX(created_at) FROM peopleflowtotals
                WHERE camera_id IN  ({camera_ids_str})
                AND VALID=1
                ORDER BY created_at ASC
            """)
        max_created_at_az = az_cursor.fetchone()[0]
    
        # 4. Find di  to import from azure
        if max_created_at_li is None:
            di = max_created_at_az - timedelta(days=100)
        else: 
            di = max_created_at_li + timedelta(days=1)

        # 5. import from azure record when  dates more recent  than dist
        az_cursor.execute("""
            SELECT *
            FROM peopleflowtotals
            WHERE camera_id IN ({camera_ids_str})
            AND created_at > ?
            ORDER BY created_at ASC
        """, di)
        rows = az_cursor.fetchall()

        # 6. Insert those records at azure
        for row in rows:
            li_cursor.execute("""
                INSERT OR REPLACE INTO peopleflowtotals 
                (created_at,camera_id,total_inside,total_outside,valid)
                VALUES (?, ?, ?, ?, ?)
            """, (row.created_at, row.camera_id, row.total_inside, row.total_outside, row.valid))
            
        # 7. Select all records in local db to df_peopleflow
        query="""
            SELECT *
            FROM peopleflowtotals
            WHERE camera_id IN ({?})
            ORDER BY created_at ASC
        """
        df_peopleflowtotals=pd.read_sql_query(query, li_conn, params=(camera_ids_str))
        li_conn.commit


        return li_conn, li_cursor,df_login_cameras,df_peopleflowtotals,camera_ids,max_created_at_az
    
    except:
        return None


# ---------------------------------------------------------
# 4. FIND FAILURES AND CORRECTION FROM BAD OR MISSED RECORDS
# ---------------------------------------------------------
def find_failures_bad_or_missing_records():
    return []

def find_corrections_bad_or_missing_records():
    return []

def find_and_correct_negative_and_excessive_totals():
    return []


# ---------------------------------------------------------
# 5. APPLY CORRECTIONS TO AZURE
# ---------------------------------------------------------
def apply_corrections_to_azure():
    return "OK"



def main():

    init_log()
    az_conn, az_cursor, client_locations=get_client_location_list()

    if client_locations==None:
        print("Nenhum Cliente a processar")

    for client,location in client_locations:
        try:
            li_conn, li_cursor,df_login_cameras,df_peopleflowtotals,camera_ids,max_created_at_az=synchro_sqlite_az(client,location,az_cursor)
            print(df_login_cameras.head(10))
            print(df_peopleflowtotals.head(10))
            print(camera_ids)
            print(max_created_at_az)
            
            failures_bad_or_missing_records=find_failures_bad_or_missing_records()
            print(failures_bad_or_missing_records)
            
            corrections_bad_or_missing_records=find_corrections_bad_or_missing_records()
            print(corrections_bad_or_missing_records)
            
            correction_negative_or_excessive_totals=find_and_correct_negative_and_excessive_totals()
            print(correction_negative_or_excessive_totals)
            
            result=apply_corrections_to_azure()
            print(result)
            
            li_cursor.close()
            li_conn.close()
        except:
            print(f"Falha ao processar client {client} location {location}")
    az_cursor.close()
    az_conn.close()

